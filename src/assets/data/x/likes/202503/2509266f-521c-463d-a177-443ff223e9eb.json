{"text": "ずっと読むの放置していたけど、知見の塊だった。\n\n特に\n\n&gt; LLMの事前学習データが不足している場合、事前学習データを4周するまでは、ユニークなデータを同じだけ学習(つまり4倍のデータで学習)する場合と比較しても、LLMに性能の差がない。（要約）\n\n知らなかった。\n\nhttps://t.co/g3F44kxfv8 https://t.co/Ge7HzZcSzm", "username": "asap2650", "tweet_url": "https://twitter.com/asap2650/status/1902738645691588955", "first_link": "http://amzn.to/4bFYX1N", "created_at": "March 20, 2025 at 03:06PM", "embed_code": "<blockquote class=\"twitter-tweet\">\n  <p lang=\"en\" dir=\"ltr\">ずっと読むの放置していたけど、知見の塊だった。\n\n特に\n\n&gt; LLMの事前学習データが不足している場合、事前学習データを4周するまでは、ユニークなデータを同じだけ学習(つまり4倍のデータで学習)する場合と比較しても、LLMに性能の差がない。（要約）\n\n知らなかった。\n\nhttps://t.co/g3F44kxfv8 https://t.co/Ge7HzZcSzm</p>\n  &mdash; asap (@asap2650)\n  <a href=\"https://twitter.com/asap2650/status/1902738645691588955\">Mar 20, 2025</a>\n</blockquote>\n<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n", "liked_at": "2025-03-21T16:41:35.779615", "source": "ifttt"}